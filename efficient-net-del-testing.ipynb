{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Do not click on \"Save Version\"\n!pip install efficientnet","metadata":{"execution":{"iopub.status.busy":"2021-12-10T13:35:49.956449Z","iopub.execute_input":"2021-12-10T13:35:49.957018Z","iopub.status.idle":"2021-12-10T13:35:59.329814Z","shell.execute_reply.started":"2021-12-10T13:35:49.956893Z","shell.execute_reply":"2021-12-10T13:35:59.328486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\n\nimport os\nimport numpy as np\nimport cv2\nimport pickle\nimport random\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential, Model, Input, model_from_json\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, concatenate, Lambda, BatchNormalization,AveragePooling2D\nfrom keras import metrics\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nimport efficientnet as efn\nimport tensorflow.keras\nimport numpy as np\nimport tensorflow as tf\nfrom keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom keras.models import Model\nfrom tensorflow.python.keras.layers import Dense,Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout, GlobalAveragePooling2D\n\n\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.mobilenet import preprocess_input,decode_predictions\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nimport efficientnet.keras as efn","metadata":{"execution":{"iopub.status.busy":"2021-12-10T13:35:59.331970Z","iopub.execute_input":"2021-12-10T13:35:59.332406Z","iopub.status.idle":"2021-12-10T13:36:06.273514Z","shell.execute_reply.started":"2021-12-10T13:35:59.332356Z","shell.execute_reply":"2021-12-10T13:36:06.272165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/covid19-radiography-database/COVID-19_Radiography_Dataset/'\n\np_path = '../input/chest-xray-pneumonia/chest_xray/train/'\n#n_path= '../input/covid19-radiography-database/COVID-19_Radiography_Dataset/Normal'\n#classes = ['Normal', 'COVID', 'Lung_Opacity', 'Viral Pneumonia']\n#classes = ['Normal', 'COVID', 'Viral Pneumonia']\nclasses0 = ['Normal']\nclasses1 = ['COVID']\nclasses2 = ['PNEUMONIA']\nX = []\ny = []\nsize = (64, 64)\n\nfor cat in range(len(classes0)):\n    #images = []\n    \n    normal_path = os.path.join(path, classes0[cat])\n    print(normal_path, ':', len(os.listdir(normal_path)))\n    for image_name in random.sample(os.listdir(normal_path), 10):\n        try:\n            img = cv2.imread(os.path.join(normal_path, image_name))\n            img = cv2.resize(img, size)\n            img = np.array(img)\n            img = np.reshape(img, (64, 64, 3))\n            img = img / 255\n            #images.append(img)\n            X.append(img)\n            y.append(0)\n        except:\n            print(os.path.join(normal_path, image_name))\n    #X.append(images)\n\nfor cat in range(len(classes1)):\n    #images = []\n    full_path = os.path.join(path, classes1[cat])\n    print(full_path, ':', len(os.listdir(full_path)))\n    for image_name in random.sample(os.listdir(full_path), 10):\n        try:\n            img = cv2.imread(os.path.join(full_path, image_name))\n            img = cv2.resize(img, size)\n            img = np.array(img)\n            img = np.reshape(img, (64, 64, 3))\n            img = img / 255\n            #images.append(img)\n            X.append(img)\n            y.append(1)\n        except:\n            print(os.path.join(full_path, image_name))\n    #X.append(images)\n\n    \nfor cat in range(len(classes2)):\n    #images = []\n    pneumonia_path = os.path.join(p_path, classes2[cat])\n    print(pneumonia_path, ':', len(os.listdir(pneumonia_path)))\n    for image_name in random.sample(os.listdir(pneumonia_path), 10):\n        try:\n            img = cv2.imread(os.path.join(pneumonia_path, image_name))\n            img = cv2.resize(img, size)\n            img = np.array(img)\n            img = np.reshape(img, (64, 64, 3))\n            img = img / 255\n            #images.append(img)\n            X.append(img)\n            y.append(2)\n        except:\n            print(os.path.join(pneumonia_path, image_name))\n    #X.append(images)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:54:11.329295Z","iopub.execute_input":"2021-12-08T03:54:11.329668Z","iopub.status.idle":"2021-12-08T03:54:12.59413Z","shell.execute_reply.started":"2021-12-08T03:54:11.329624Z","shell.execute_reply":"2021-12-08T03:54:12.592955Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_pairs(x, y, p=1):\n    num_classes = 3\n    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n\n    pairs = []\n    labels = []\n\n    for idx1 in range(len(x)):\n        for _ in range(p):\n            # add a matching example\n            x1 = x[idx1]\n            #print(y[idx1])\n            label1 = y[idx1]\n            #print(label1[0].shape, type(label1[0][0]))\n            #print(digit_indices[label1])\n            idx2 = random.choice(digit_indices[label1])\n            x2 = x[idx2]\n\n            pairs += [[x1, x2]]\n            labels += [1]\n\n            # add a non-matching example\n            label2 = random.randint(0, num_classes - 1)\n            while label2 == label1:\n                label2 = random.randint(0, num_classes - 1)\n\n            idx2 = random.choice(digit_indices[label2])\n            x2 = x[idx2]\n\n            pairs += [[x1, x2]]\n            labels += [0]\n\n    return np.array(pairs), np.array(labels).astype(\"float32\")\n\n\n# pairs_train, labels_train = make_pairs(x_train, y_train)\n# pairs_test, labels_test = make_pairs(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:54:12.596059Z","iopub.execute_input":"2021-12-08T03:54:12.596374Z","iopub.status.idle":"2021-12-08T03:54:12.606192Z","shell.execute_reply.started":"2021-12-08T03:54:12.596339Z","shell.execute_reply":"2021-12-08T03:54:12.605062Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_pairs(x, y):\n    pairs = [[], []]\n    labels = []\n    \n    for idx1 in range(len(x)):\n        for idx2 in range(len(x)):\n            pairs[0].append(x[idx1])\n            pairs[1].append(x[idx2])\n            labels.append(y[idx1] == y[idx2])\n    \n    return np.array(pairs), np.array(labels).astype(\"float32\")\n\npairs_train, labels_train = make_pairs(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:54:12.607912Z","iopub.execute_input":"2021-12-08T03:54:12.608287Z","iopub.status.idle":"2021-12-08T03:54:12.681166Z","shell.execute_reply.started":"2021-12-08T03:54:12.608254Z","shell.execute_reply":"2021-12-08T03:54:12.680139Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def change_model(model, new_input_shape=(None, 64, 64, 3),custom_objects=None):\n    # replace input shape of first layer\n    \n    config = model.layers[0].get_config()\n    config['batch_input_shape']=new_input_shape\n    model._layers[0]=model.layers[0].from_config(config)\n\n    # rebuild model architecture by exporting and importing via json\n    new_model = tensorflow.keras.models.model_from_json(model.to_json(),custom_objects=custom_objects)\n\n    # copy weights from old model to new one\n    for layer in new_model._layers:\n        try:\n            layer.set_weights(model.get_layer(name=layer.name).get_weights())\n            print(\"Loaded layer {}\".format(layer.name))\n        except:\n            print(\"Could not transfer weights for layer {}\".format(layer.name))\n\n    return new_model","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:54:12.683188Z","iopub.execute_input":"2021-12-08T03:54:12.683548Z","iopub.status.idle":"2021-12-08T03:54:12.691627Z","shell.execute_reply.started":"2021-12-08T03:54:12.683508Z","shell.execute_reply":"2021-12-08T03:54:12.6904Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def euclidean_distance(vects):\n    x, y = vects\n    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n\ndef loss(margin=1):\n    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n\n  Arguments:\n      margin: Integer, defines the baseline for distance for which pairs\n              should be classified as dissimilar. - (default is 1).\n\n  Returns:\n      'constrastive_loss' function with data ('margin') attached.\n  \"\"\"\n\n    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n    #                         true_value * square( max(margin-prediction, 0) ))\n    def contrastive_loss(y_true, y_pred):\n        \"\"\"Calculates the constrastive loss.\n\n      Arguments:\n          y_true: List of labels, each label is of type float32.\n          y_pred: List of predictions of same length as of y_true,\n                  each label is of type float32.\n\n      Returns:\n          A tensor containing constrastive loss as floating point value.\n      \"\"\"\n\n        square_pred = tf.math.square(y_pred)\n        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n        return tf.math.reduce_mean(\n            (1 - y_true) * square_pred + (y_true) * margin_square\n        )\n\n    return contrastive_loss","metadata":{"execution":{"iopub.status.busy":"2021-12-09T15:41:25.339379Z","iopub.execute_input":"2021-12-09T15:41:25.34021Z","iopub.status.idle":"2021-12-09T15:41:25.35206Z","shell.execute_reply.started":"2021-12-09T15:41:25.340148Z","shell.execute_reply":"2021-12-09T15:41:25.350722Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = efn.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(64,64,3))\nnew_model = change_model(base_model, new_input_shape=[None] + [64,64,3])\nnew_model.summary()\nx = new_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\nembeddings = Dense(128, activation='relu')(x)\n# predictions = Dense(3, activation='sigmoid')(x)\n\n# embedding_network = keras.Model(new_model.input, outputs=predictions)\nembedding_network = keras.Model(new_model.input, outputs=embeddings)\n\ninput_1 = Input((64, 64, 3))\ninput_2 = Input((64, 64, 3))\n\ntower_1 = embedding_network(input_1)\ntower_2 = embedding_network(input_2)\n\nmerge_layer = Lambda(euclidean_distance)([tower_1, tower_2])\nnormal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\noutput_layer = Dense(1, activation=\"sigmoid\")(normal_layer)\nsiamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:54:12.712158Z","iopub.execute_input":"2021-12-08T03:54:12.71249Z","iopub.status.idle":"2021-12-08T03:54:18.249154Z","shell.execute_reply.started":"2021-12-08T03:54:12.712461Z","shell.execute_reply":"2021-12-08T03:54:18.248057Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\nbatch_size = 3\nmargin = 1  # Margin for constrastive loss.","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:54:18.250573Z","iopub.execute_input":"2021-12-08T03:54:18.25089Z","iopub.status.idle":"2021-12-08T03:54:18.255227Z","shell.execute_reply.started":"2021-12-08T03:54:18.250861Z","shell.execute_reply":"2021-12-08T03:54:18.254241Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\nsiamese.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:54:18.256365Z","iopub.execute_input":"2021-12-08T03:54:18.256647Z","iopub.status.idle":"2021-12-08T03:54:18.305899Z","shell.execute_reply.started":"2021-12-08T03:54:18.256618Z","shell.execute_reply":"2021-12-08T03:54:18.305056Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs_train.shape, labels_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:54:50.798533Z","iopub.execute_input":"2021-12-08T03:54:50.798879Z","iopub.status.idle":"2021-12-08T03:54:50.808842Z","shell.execute_reply.started":"2021-12-08T03:54:50.79885Z","shell.execute_reply":"2021-12-08T03:54:50.807525Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = siamese.fit((pairs_train[0], pairs_train[1]), labels_train, epochs=epochs, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:55:10.397031Z","iopub.execute_input":"2021-12-08T03:55:10.397735Z","iopub.status.idle":"2021-12-08T04:02:08.958444Z","shell.execute_reply.started":"2021-12-08T03:55:10.397698Z","shell.execute_reply":"2021-12-08T04:02:08.95768Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(\n    siamese, to_file='siamese_nimai_model.png', show_shapes=True, show_dtype=True,\n    show_layer_names=True, rankdir='TB', expand_nested=True, dpi=96\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T03:17:37.817951Z","iopub.execute_input":"2021-06-05T03:17:37.818247Z","iopub.status.idle":"2021-06-05T03:17:40.606927Z","shell.execute_reply.started":"2021-06-05T03:17:37.818217Z","shell.execute_reply":"2021-06-05T03:17:40.605537Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load json and create model\njson_file = open('../input/efficientnet/best_siamese_efficientB0_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"../input/efficientnet/best_siamese_efficientB0.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:07:36.814724Z","iopub.execute_input":"2021-12-09T17:07:36.815187Z","iopub.status.idle":"2021-12-09T17:07:40.184543Z","shell.execute_reply.started":"2021-12-09T17:07:36.815143Z","shell.execute_reply":"2021-12-09T17:07:40.183332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load json and create model\njson_file = open('../input/efficientnet/vgg16_siamese_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"../input/efficientnet/vgg16_constrastive_loss.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:33:02.697204Z","iopub.execute_input":"2021-06-12T12:33:02.697699Z","iopub.status.idle":"2021-06-12T12:33:04.095992Z","shell.execute_reply.started":"2021-06-12T12:33:02.697652Z","shell.execute_reply":"2021-06-12T12:33:04.094961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(\n    loaded_model, to_file='loaded_siamese_nimai_model.png', show_shapes=True, show_dtype=True,\n    show_layer_names=True, rankdir='TB', expand_nested=True, dpi=96\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:33:04.098275Z","iopub.execute_input":"2021-06-12T12:33:04.098614Z","iopub.status.idle":"2021-06-12T12:33:04.901747Z","shell.execute_reply.started":"2021-06-12T12:33:04.098579Z","shell.execute_reply":"2021-06-12T12:33:04.900396Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(X)\ny = np.array(y)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:40:08.098815Z","iopub.execute_input":"2021-12-08T02:40:08.09945Z","iopub.status.idle":"2021-12-08T02:40:08.107792Z","shell.execute_reply.started":"2021-12-08T02:40:08.09941Z","shell.execute_reply":"2021-12-08T02:40:08.106582Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:40:12.263458Z","iopub.execute_input":"2021-12-08T02:40:12.263905Z","iopub.status.idle":"2021-12-08T02:40:12.273434Z","shell.execute_reply.started":"2021-12-08T02:40:12.26387Z","shell.execute_reply":"2021-12-08T02:40:12.27204Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:33:05.388883Z","iopub.execute_input":"2021-06-12T12:33:05.389408Z","iopub.status.idle":"2021-06-12T12:33:05.854075Z","shell.execute_reply.started":"2021-06-12T12:33:05.389359Z","shell.execute_reply":"2021-06-12T12:33:05.852893Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\ny_train = y_train.reshape((y_train.shape[0], 1))\ny_test = y_test.reshape((y_test.shape[0], 1))\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:58:40.541406Z","iopub.execute_input":"2021-06-09T03:58:40.541828Z","iopub.status.idle":"2021-06-09T03:58:40.554013Z","shell.execute_reply.started":"2021-06-09T03:58:40.541795Z","shell.execute_reply":"2021-06-09T03:58:40.552743Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = x_train.astype(\"float32\")\nx_test = x_test.astype(\"float32\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T09:28:21.047266Z","iopub.execute_input":"2021-06-10T09:28:21.047887Z","iopub.status.idle":"2021-06-10T09:28:21.26292Z","shell.execute_reply.started":"2021-06-10T09:28:21.047837Z","shell.execute_reply":"2021-06-10T09:28:21.261947Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'              precision    recall  f1-score   support\n'       0.0       0.96      0.98      0.97      2199\n'       1.0       0.98      0.96      0.97      2199\n'accuracy                             0.97      4398\n'macro avg        0.97      0.97      0.97      4398\n'weighted avg     0.97      0.97      0.97      4398","metadata":{}},{"cell_type":"code","source":"digit_indices = [np.where(y_train == i)[0] for i in range(3)]\nlen(digit_indices), len(digit_indices[0]), len(digit_indices[1]), len(digit_indices[2])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:10:00.528233Z","iopub.execute_input":"2021-06-10T15:10:00.528631Z","iopub.status.idle":"2021-06-10T15:10:00.5377Z","shell.execute_reply.started":"2021-06-10T15:10:00.528588Z","shell.execute_reply":"2021-06-10T15:10:00.536689Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nclasses = [\n    'Atelectasis',\n    'Cardiomegaly',\n    'Consolidation',\n    'Edema',\n    'Effusion',\n    'Emphysema',\n    'Fibrosis',\n    'Hernia',\n    'Infiltration',\n    'Pleural_Thickening',\n    'Pneumothorax',\n    'Pneumonia',\n    'Nodule',\n    'Mass',\n    'No Finding'\n]\n\nmeta_data = pd.read_csv('/kaggle/input/processeddataforlungdiseases/preprocessed_data.csv')\nfor i, d in enumerate(classes):\n    print(i, d, len(meta_data[meta_data[d] == 1]), len(meta_data[meta_data[d] == 0]))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:18:33.343467Z","iopub.execute_input":"2021-12-09T17:18:33.343916Z","iopub.status.idle":"2021-12-09T17:18:33.722505Z","shell.execute_reply.started":"2021-12-09T17:18:33.34388Z","shell.execute_reply":"2021-12-09T17:18:33.72145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_data.columns\nmeta_data['Unnamed: 0'][:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T16:23:28.661511Z","iopub.execute_input":"2021-12-09T16:23:28.662351Z","iopub.status.idle":"2021-12-09T16:23:28.673007Z","shell.execute_reply.started":"2021-12-09T16:23:28.662259Z","shell.execute_reply":"2021-12-09T16:23:28.671782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = [[]] * len(classes)\nsize = (64, 64)\ndataset_len = 5000\ntest_size = 200\n\nfor i in range(len(classes)):\n    y[i] = list(meta_data[:dataset_len][classes[i]])\ny = np.array(y).T","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:18:40.065746Z","iopub.execute_input":"2021-12-09T17:18:40.066085Z","iopub.status.idle":"2021-12-09T17:18:40.088594Z","shell.execute_reply.started":"2021-12-09T17:18:40.066054Z","shell.execute_reply":"2021-12-09T17:18:40.087502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:18:42.208673Z","iopub.execute_input":"2021-12-09T17:18:42.209011Z","iopub.status.idle":"2021-12-09T17:18:42.21545Z","shell.execute_reply.started":"2021-12-09T17:18:42.208981Z","shell.execute_reply":"2021-12-09T17:18:42.214332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nX = []\nfor path in tqdm(meta_data[:dataset_len]['Unnamed: 0'], total=dataset_len):\n    try:\n        img = cv2.imread(path)\n        img = cv2.resize(img, size)\n        img = np.array(img)\n        img = np.reshape(img, (64, 64, 3))\n        img = img / 255\n\n        X.append(img)\n    except:\n        print(os.path.join(normal_path, image_name))\nX = np.array(X)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:12:40.609575Z","iopub.execute_input":"2021-12-09T17:12:40.609914Z","iopub.status.idle":"2021-12-09T17:14:51.478327Z","shell.execute_reply.started":"2021-12-09T17:12:40.609884Z","shell.execute_reply":"2021-12-09T17:14:51.477135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shots = 8\npreds = []\naccuracies = []\n\nsupport_set_indices_positive = []\nsupport_set_indices_negative = []\nsupport_set_positive = []\nsupport_set_negative = []\n\nfor cat in range(len(classes)):\n    support_set_indices_positive.append(random.sample(list(np.where(y[:, cat] == 1)[0]), shots))\n    support_set_positive.append(list(X[support_set_indices_positive[-1]]))\n    support_set_indices_negative.append(random.sample(list(np.where(y[:, cat] == 0)[0]), shots))\n    support_set_negative.append(list(X[support_set_indices_negative[-1]]))\nsupport_set_positive = np.array(support_set_positive)\nsupport_set_negative = np.array(support_set_negative)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:19:11.100559Z","iopub.execute_input":"2021-12-09T17:19:11.101145Z","iopub.status.idle":"2021-12-09T17:19:11.130474Z","shell.execute_reply.started":"2021-12-09T17:19:11.101095Z","shell.execute_reply":"2021-12-09T17:19:11.129721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\ntest_set = random.sample(range(len(X)), test_size)\ncorrect_pred = [0] * len(classes)\n\nfor i in tqdm(test_set, total=test_size):\n    temp = np.repeat([X[i]], shots, axis=0)\n    pred = []\n    for cat in range(len(classes)):\n        pred_positive = np.sum(np.array(loaded_model.predict([support_set_positive[cat], temp])))\n        pred_negative = np.sum(np.array(loaded_model.predict([support_set_negative[cat], temp])))\n        pred_temp = int(pred_positive > pred_negative)\n        pred.append(pred_temp)\n        correct_pred[cat] += pred_temp == y[i][cat]\n    preds.append([pred])\n#     print(pred, y[i])\npreds = np.array(preds).T\n# print(preds)\nprint(np.array(correct_pred)/test_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:19:15.601035Z","iopub.execute_input":"2021-12-09T17:19:15.601363Z","iopub.status.idle":"2021-12-09T17:28:59.990281Z","shell.execute_reply.started":"2021-12-09T17:19:15.601334Z","shell.execute_reply":"2021-12-09T17:28:59.989097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for r in zip(classes, np.array(correct_pred)/test_size):\n    print(r)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:34:08.691478Z","iopub.execute_input":"2021-12-09T17:34:08.691891Z","iopub.status.idle":"2021-12-09T17:34:08.700044Z","shell.execute_reply.started":"2021-12-09T17:34:08.691856Z","shell.execute_reply":"2021-12-09T17:34:08.698877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_size = len(X)\nshots = 8\npreds = []\naccuracies = []\n\nfor shot in range(1, shots+1):\n    normal = random.sample(list(digit_indices[0]), shot)\n    covid = random.sample(list(digit_indices[1]), shot)\n    pneumonia = random.sample(list(digit_indices[2]), shot)\n    \n    normal_imgs = x_train[normal]\n    covid_imgs = x_train[covid]\n    pneumonia_imgs = x_train[pneumonia]\n    \n    temp_pred = []\n    \n    for test_img in tqdm(x_test[:test_size], total=test_size):\n        temp = np.repeat([test_img], shot, axis=0)\n        # print(normal_imgs.shape, temp.shape)\n        pred_0 = np.array(loaded_model.predict([normal_imgs, temp]) > 0.5)\n        # print(pred_0)\n        pred_0 = np.sum(pred_0)\n        # print(pred_0)\n        \n        pred_1 = np.array(loaded_model.predict([covid_imgs, temp]) > 0.5)\n        # print(pred_1)\n        pred_1 = np.sum(pred_1)\n        # print(pred_1)\n        \n        pred_2 = np.array(loaded_model.predict([pneumonia_imgs, temp]) > 0.5)\n        # print(pred_2)\n        pred_2 = np.sum(pred_2)\n        # print(pred_2)\n        \n        # print(np.argmax([pred_0, pred_1, pred_2]))\n        temp_pred.append(np.argmax([pred_0, pred_1, pred_2]))\n        \n        # print()\n    # print('preds', preds)\n    # print('y_test', y_test[:test_size])\n    print(shot, 'shot accuracy =', (np.sum(temp_pred == y_test[:test_size]) * 100) / test_size)\n    preds.append(temp_pred)\n    accuracies.append((np.sum(temp_pred == y_test[:test_size]) * 100) / test_size)\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T06:45:41.714074Z","iopub.execute_input":"2021-06-05T06:45:41.714498Z","iopub.status.idle":"2021-06-05T07:41:28.550749Z","shell.execute_reply.started":"2021-06-05T06:45:41.714458Z","shell.execute_reply":"2021-06-05T07:41:28.549911Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for p in preds:\n    print()\n    print(confusion_matrix(y_test, p))\n    print()\n    print(classification_report(y_test, p))\n    print()\n    \n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199\n\n\n\n# [[685  20   7]\n#  [ 13 717   2]\n#  [  1   3 751]]\n\n#               precision    recall  f1-score   support\n\n#            0       0.98      0.96      0.97       712\n#            1       0.97      0.98      0.97       732\n#            2       0.99      0.99      0.99       755\n\n#     accuracy                           0.98      2199\n#    macro avg       0.98      0.98      0.98      2199\n# weighted avg       0.98      0.98      0.98      2199","metadata":{"execution":{"iopub.status.busy":"2021-06-05T07:55:49.904453Z","iopub.execute_input":"2021-06-05T07:55:49.904759Z","iopub.status.idle":"2021-06-05T07:55:50.026531Z","shell.execute_reply.started":"2021-06-05T07:55:49.90473Z","shell.execute_reply":"2021-06-05T07:55:50.025751Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199\n\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97       712\n           1       0.97      0.98      0.97       732\n           2       0.99      0.99      0.99       755\n\n    accuracy                           0.98      2199\n   macro avg       0.98      0.98      0.98      2199\nweighted avg       0.98      0.98      0.98      2199","metadata":{}},{"cell_type":"code","source":"accuracies\n# [97.9081400636653,\n#  97.9081400636653,\n#  97.9081400636653,\n#  97.9081400636653,\n#  97.9081400636653,\n#  97.9081400636653,\n#  97.9081400636653,\n#  97.9081400636653,\n#  97.9081400636653,\n#  97.9081400636653]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T07:52:30.842013Z","iopub.execute_input":"2021-06-05T07:52:30.842321Z","iopub.status.idle":"2021-06-05T07:52:30.847457Z","shell.execute_reply.started":"2021-06-05T07:52:30.842294Z","shell.execute_reply":"2021-06-05T07:52:30.846638Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_size = len(x_test)\nshot = 8\n\nnormal = random.sample(list(digit_indices[0]), shot)\ncovid = random.sample(list(digit_indices[1]), shot)\npneumonia = random.sample(list(digit_indices[2]), shot)\n\nnormal_imgs = x_train[normal]\ncovid_imgs = x_train[covid]\npneumonia_imgs = x_train[pneumonia]\n\npred_vals = []\n\nfor test_img in tqdm(x_test[:test_size], total=test_size):\n    temp = np.repeat([test_img], shot, axis=0)\n    pred_0 = np.array(loaded_model.predict([normal_imgs, temp]))\n#     print(pred_0)\n\n    pred_1 = np.array(loaded_model.predict([covid_imgs, temp]))\n#     print(pred_1)\n\n    pred_2 = np.array(loaded_model.predict([pneumonia_imgs, temp]))\n#     print(pred_2)\n    \n    pred_vals.append(np.array([pred_0, pred_1, pred_2]))\n#     print()\n\npred_vals = np.array(pred_vals)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:34:00.364003Z","iopub.execute_input":"2021-06-09T03:34:00.364482Z","iopub.status.idle":"2021-06-09T03:41:55.921902Z","shell.execute_reply.started":"2021-06-09T03:34:00.364424Z","shell.execute_reply":"2021-06-09T03:41:55.920332Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pred_vals.shape)\nsum1 = np.sum(pred_vals[:4], axis=2)/8\nprint(sum1.shape)\nohe = OneHotEncoder()\ntemp = ohe.fit_transform(y_test).toarray()\n# for i in range(4):\n#     print(temp[i], sum1[i])\nprint(temp.shape)\ntemp = temp.reshape((temp.shape[0], temp.shape[1], 1))\nprint(temp.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T04:01:20.151408Z","iopub.execute_input":"2021-06-09T04:01:20.151749Z","iopub.status.idle":"2021-06-09T04:01:20.162615Z","shell.execute_reply.started":"2021-06-09T04:01:20.151718Z","shell.execute_reply":"2021-06-09T04:01:20.161332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normal vs not normal\ntpr = []\nfpr = []\nstep = 0.00005\nfor threshold in tqdm(np.arange(0, 1+step, step), total=len(np.arange(0, 1+step, step))):\n    preds = []\n    temp = pred_vals[:] >= threshold\n    # print(temp)\n    temp = np.sum(temp, axis=2)\n    # print(temp)\n    # print(np.argmax(temp, axis=1))\n    preds = np.argmax(temp, axis=1)\n    # print(preds)\n    preds_normal = preds != 0\n    # print(preds)\n\n    cm = confusion_matrix(y_test != 0, preds_normal)\n    tpr.append(cm[0][0] / (cm[0][0] + cm[0][1]))\n    fpr.append(cm[1][0] / (cm[1][0] + cm[1][1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T11:35:56.254368Z","iopub.execute_input":"2021-06-08T11:35:56.254685Z","iopub.status.idle":"2021-06-08T11:36:42.755732Z","shell.execute_reply.started":"2021-06-08T11:35:56.254655Z","shell.execute_reply":"2021-06-08T11:36:42.754755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test != 0, preds_normal)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T11:23:15.550897Z","iopub.execute_input":"2021-06-08T11:23:15.551243Z","iopub.status.idle":"2021-06-08T11:23:15.563704Z","shell.execute_reply.started":"2021-06-08T11:23:15.551212Z","shell.execute_reply":"2021-06-08T11:23:15.562777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.grid()\nplt.plot(fpr, tpr, 'ro--')\nplt.plot([0,1], [0,1], '--')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T11:36:42.757239Z","iopub.execute_input":"2021-06-08T11:36:42.75758Z","iopub.status.idle":"2021-06-08T11:36:42.946338Z","shell.execute_reply.started":"2021-06-08T11:36:42.75754Z","shell.execute_reply":"2021-06-08T11:36:42.94541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pred_vals[:] >= 0.9999\npredictions = np.sum(predictions, axis=2)\npredictions = np.argmax(predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T06:25:59.067599Z","iopub.execute_input":"2021-06-07T06:25:59.067972Z","iopub.status.idle":"2021-06-07T06:25:59.07388Z","shell.execute_reply.started":"2021-06-07T06:25:59.067927Z","shell.execute_reply":"2021-06-07T06:25:59.072852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = predictions.reshape(predictions.shape[0])\npredictions.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-07T06:26:06.418381Z","iopub.execute_input":"2021-06-07T06:26:06.418709Z","iopub.status.idle":"2021-06-07T06:26:06.424434Z","shell.execute_reply.started":"2021-06-07T06:26:06.418682Z","shell.execute_reply":"2021-06-07T06:26:06.423634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y, predictions), np.sum(y == predictions) * 100 / len(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T06:26:08.315266Z","iopub.execute_input":"2021-06-07T06:26:08.315622Z","iopub.status.idle":"2021-06-07T06:26:08.325276Z","shell.execute_reply.started":"2021-06-07T06:26:08.315589Z","shell.execute_reply":"2021-06-07T06:26:08.32438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"digit_indices_support = [np.where(y_train == i)[0] for i in range(3)]\nprint(len(digit_indices_support), len(digit_indices_support[0]), len(digit_indices_support[1]), len(digit_indices_support[2]))\n\ndigit_indices_test = [np.where(y_test == i)[0] for i in range(3)]\nprint(len(digit_indices_test), len(digit_indices_test[0]), len(digit_indices_test[1]), len(digit_indices_test[2]))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T09:47:42.675604Z","iopub.execute_input":"2021-06-10T09:47:42.675922Z","iopub.status.idle":"2021-06-10T09:47:42.686415Z","shell.execute_reply.started":"2021-06-10T09:47:42.675891Z","shell.execute_reply":"2021-06-10T09:47:42.685371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size = len(x_test)\nshot = 8\n\nnormal = random.sample(list(digit_indices_support[0]), shot)\ncovid = random.sample(list(digit_indices_support[1]), shot)\npneumonia = random.sample(list(digit_indices_support[2]), shot)\n\nnormal_imgs = x_train[normal]\ncovid_imgs = x_train[covid]\npneumonia_imgs = x_train[pneumonia]\n\npred_vals = []\n\nfor test_img in tqdm(x_test[np.concatenate((digit_indices_test[1], digit_indices_test[2]))][:], total=len(np.concatenate((digit_indices_test[1], digit_indices_test[2])))):\n    temp = np.repeat([test_img], shot, axis=0)\n    \n    # pred_0 = np.array(loaded_model.predict([normal_imgs, temp]))\n    # print(pred_0)\n    # print(pred_0.shape)\n    # pred_0 = np.sum(pred_0) / shot\n    # print(pred_0)\n    # print(pred_0.shape)\n    # pred_vals.append(pred_0)\n\n    # pred_1 = np.array(loaded_model.predict([covid_imgs, temp]))\n    # print(pred_1)\n    # print(pred_1.shape)\n    # pred_1 = np.sum(pred_1) / shot\n    # print(pred_1)\n    # print(pred_1.shape)\n    # pred_vals.append(pred_1)\n\n    pred_2 = np.array(loaded_model.predict([pneumonia_imgs, temp]))\n    # print(pred_2)\n    # print(pred_2.shape)\n    pred_2 = np.sum(pred_2) / shot\n    # print(pred_2)\n    # print(pred_2.shape)\n    pred_vals.append(pred_2)\n    \n    # print()\n\npred_vals = np.array(pred_vals)\npred_vals.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-10T09:57:28.635439Z","iopub.execute_input":"2021-06-10T09:57:28.635885Z","iopub.status.idle":"2021-06-10T09:58:45.06981Z","shell.execute_reply.started":"2021-06-10T09:57:28.635841Z","shell.execute_reply":"2021-06-10T09:58:45.069043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_test[:20])\nprint(y_test[np.concatenate((digit_indices_test[1], digit_indices_test[2]))][:20])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T09:55:01.469638Z","iopub.execute_input":"2021-06-10T09:55:01.469997Z","iopub.status.idle":"2021-06-10T09:55:01.476211Z","shell.execute_reply.started":"2021-06-10T09:55:01.469958Z","shell.execute_reply":"2021-06-10T09:55:01.475235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_pairs_test(x_train, y_train, x_test, y_test, p=1):\n    num_classes = 3\n    digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\n\n    pairs = []\n    labels = []\n\n    for idx1 in range(len(x_test)):\n        for _ in range(p):\n            # add a matching example\n            x1 = x_test[idx1]\n            #print(y[idx1])\n            label1 = y_test[idx1]\n            #print(label1[0].shape, type(label1[0][0]))\n            #print(digit_indices[label1])\n            idx2 = random.choice(digit_indices[label1])\n            x2 = x_train[idx2]\n\n            pairs += [[x1, x2]]\n            labels += [1]\n\n            # add a non-matching example\n            label2 = random.randint(0, num_classes - 1)\n            while label2 == label1:\n                label2 = random.randint(0, num_classes - 1)\n\n            idx2 = random.choice(digit_indices[label2])\n            x2 = x_train[idx2]\n\n            pairs += [[x1, x2]]\n            labels += [0]\n\n    return np.array(pairs), np.array(labels).astype(\"float32\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:33:05.855883Z","iopub.execute_input":"2021-06-12T12:33:05.856338Z","iopub.status.idle":"2021-06-12T12:33:05.867436Z","shell.execute_reply.started":"2021-06-12T12:33:05.85629Z","shell.execute_reply":"2021-06-12T12:33:05.866168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = make_pairs_test(x_train, y_train, x_test, y_test)\n\nx1 = X[:, 0]\nx2 = X[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:33:05.86923Z","iopub.execute_input":"2021-06-12T12:33:05.869697Z","iopub.status.idle":"2021-06-12T12:33:06.267889Z","shell.execute_reply.started":"2021-06-12T12:33:05.869652Z","shell.execute_reply":"2021-06-12T12:33:06.266948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1.shape, x2.shape, y. shape","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:33:06.269363Z","iopub.execute_input":"2021-06-12T12:33:06.269836Z","iopub.status.idle":"2021-06-12T12:33:06.276716Z","shell.execute_reply.started":"2021-06-12T12:33:06.26979Z","shell.execute_reply":"2021-06-12T12:33:06.275477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n#  = np.array(y_test[np.concatenate((digit_indices_test[1], digit_indices_test[2]))] == 1, dtype='int')\nfpr, tpr, thresholds_keras = roc_curve(y_test[np.concatenate((digit_indices_test[1], digit_indices_test[2]))] == 2, pred_vals)\nauc_val = auc(fpr, tpr)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T09:58:45.071247Z","iopub.execute_input":"2021-06-10T09:58:45.071669Z","iopub.status.idle":"2021-06-10T09:58:45.078032Z","shell.execute_reply.started":"2021-06-10T09:58:45.071626Z","shell.execute_reply":"2021-06-10T09:58:45.077092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\ny_pred = loaded_model.predict([x1, x2])\nfpr, tpr, thresholds_keras = roc_curve(y, y_pred)\nauc_val = auc(fpr, tpr)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:33:06.278478Z","iopub.execute_input":"2021-06-12T12:33:06.279008Z","iopub.status.idle":"2021-06-12T12:36:14.854933Z","shell.execute_reply.started":"2021-06-12T12:33:06.278958Z","shell.execute_reply":"2021-06-12T12:36:14.853823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.grid()\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='area = {:.3f}'.format(auc_val))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:36:14.85652Z","iopub.execute_input":"2021-06-12T12:36:14.856839Z","iopub.status.idle":"2021-06-12T12:36:15.108589Z","shell.execute_reply.started":"2021-06-12T12:36:14.856801Z","shell.execute_reply":"2021-06-12T12:36:15.107343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y, y_pred > 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:37:13.579768Z","iopub.execute_input":"2021-06-12T12:37:13.580165Z","iopub.status.idle":"2021-06-12T12:37:13.598037Z","shell.execute_reply.started":"2021-06-12T12:37:13.580133Z","shell.execute_reply":"2021-06-12T12:37:13.597182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load json and create model\njson_file = open('../input/efficientnet/best_embedding_efficientB0_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"../input/efficientnet/best_embedding_efficientB0.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T09:04:27.96801Z","iopub.execute_input":"2021-06-17T09:04:27.968392Z","iopub.status.idle":"2021-06-17T09:04:30.640238Z","shell.execute_reply.started":"2021-06-17T09:04:27.968362Z","shell.execute_reply":"2021-06-17T09:04:30.639416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(\n    loaded_model, to_file='loaded_siamese_nimai_model.png', show_shapes=True, show_dtype=True,\n    show_layer_names=True, rankdir='TB', expand_nested=True, dpi=96\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T09:04:32.956479Z","iopub.execute_input":"2021-06-17T09:04:32.956836Z","iopub.status.idle":"2021-06-17T09:04:35.794814Z","shell.execute_reply.started":"2021-06-17T09:04:32.956795Z","shell.execute_reply":"2021-06-17T09:04:35.794047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}